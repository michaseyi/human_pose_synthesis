{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from data import default_skeleton, hierarchical_order\n",
    "from transforms3d.euler import euler2mat\n",
    "from typing import Optional\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardKinematics(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bones = nn.ModuleDict({\n",
    "            name: Bone(bone.axis, bone.direction, bone.length)\n",
    "            for (name, bone) in default_skeleton.bone_map.items()\n",
    "            if not name == \"root\"\n",
    "        })\n",
    "    def forward(self, x):\n",
    "        # (B, 282) -> (B, 93)\n",
    "        bone_cache = {}\n",
    "        i = 0\n",
    "        for bone in hierarchical_order:\n",
    "            if bone == \"root\":\n",
    "                tail_position = x[:, i:i+3]\n",
    "                i += 3\n",
    "                global_rotation = x[:, i:i + 9].view(-1, 3, 3)\n",
    "                i += 9\n",
    "                bone_cache[bone] = (global_rotation, tail_position)\n",
    "            else:\n",
    "                parent = default_skeleton.bone_map[bone].parent\n",
    "                assert parent is not None\n",
    "                assert parent.name in bone_cache\n",
    "                local_rotation = x[:, i: i + 9].view(-1, 3, 3)\n",
    "                i += 9\n",
    "                bone_cache[bone] = self.bones[bone](\n",
    "                    *bone_cache[parent.name], local_rotation)\n",
    "\n",
    "        return torch.cat([bone_cache[bone][1] for bone in hierarchical_order], dim=1)\n",
    "\n",
    "class Bone(nn.Module):\n",
    "    def __init__(self, axis, direction, length):\n",
    "        super().__init__()\n",
    "        axis = torch.tensor(axis).deg2rad().to(torch.float32)\n",
    "        bind = torch.tensor(euler2mat(*axis.tolist())).to(torch.float32)\n",
    "        inverse_bind = bind.inverse().to(torch.float32)\n",
    "        direction = torch.tensor(direction).to(torch.float32)\n",
    "        length = torch.tensor(length).to(torch.float32)\n",
    "        self.register_buffer(\"bind\", bind)\n",
    "        self.register_buffer(\"inverse_bind\", inverse_bind)\n",
    "        self.register_buffer(\"direction\", direction)\n",
    "        self.register_buffer(\"length\", length)\n",
    "\n",
    "    def forward(self, parent_global_transform, parent_tail_position, local_rotation):\n",
    "        global_transform = parent_global_transform @ self.bind @ local_rotation @ self.inverse_bind\n",
    "        tail_position = parent_tail_position + self.length * \\\n",
    "            (global_transform @ self.direction)\n",
    "        return global_transform, tail_position\n",
    "\n",
    "class FeedFowardBlock(nn.Module):\n",
    "    def __init__(self, input_embedding_size: int, hidden_embedding_size: int, output_embedding_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_embedding_size, hidden_embedding_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_embedding_size, output_embedding_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mlp(x)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_embedding_size: int,\n",
    "        hidden_embedding_size: int,\n",
    "        output_embedding_size: int,\n",
    "        attention_head_count: int,\n",
    "        dropout: float,\n",
    "        device='cpu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(input_embedding_size)\n",
    "        self.ln_2 = nn.LayerNorm(input_embedding_size)\n",
    "\n",
    "        self.mlp = FeedFowardBlock(\n",
    "            input_embedding_size, hidden_embedding_size, output_embedding_size, dropout)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            input_embedding_size, attention_head_count, dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key_value: Optional[torch.Tensor] = None, mask = True) -> torch.Tensor:\n",
    "        length = query.size(1)\n",
    "\n",
    "        attn_mask = torch.triu(torch.ones(length, length) *\n",
    "                          float('-inf'), diagonal=1).to(self.device) if mask else None\n",
    "\n",
    "        key_value = key_value if key_value is not None else query\n",
    "\n",
    "        x, _ = self.attn(self.ln_1(query), self.ln_1(key_value), self.ln_1(key_value), attn_mask=attn_mask)\n",
    "\n",
    "        x = self.mlp(self.ln_2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        number_of_layers: int,\n",
    "        input_embedding_size: int,\n",
    "        hidden_embedding_size: int,\n",
    "        output_embedding_size: int,\n",
    "        attention_head_count: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([AttentionBlock(input_embedding_size, \n",
    "                                                    hidden_embedding_size,\n",
    "                                                    input_embedding_size, \n",
    "                                                    attention_head_count, \n",
    "                                                    dropout) for _ in range(number_of_layers)])\n",
    "        \n",
    "        self.ln = nn.LayerNorm(input_embedding_size)\n",
    "        self.projection = FeedFowardBlock(input_embedding_size, hidden_embedding_size, output_embedding_size, dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, encoder_input:Optional[torch.Tensor] = None, mask=True) -> torch.Tensor:\n",
    "        x_ = x\n",
    "        for layer in self.layers:\n",
    "            x_ = layer(x_, key_value=encoder_input, mask=mask)\n",
    "        \n",
    "        x_ = self.ln(x_)\n",
    "        x_ = x + x_\n",
    "        x_ = self.projection(x_) \n",
    "        return x_\n",
    "\n",
    "\n",
    "class Denoiser(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_layers: int = 16,\n",
    "        decoder_layers: int = 16,\n",
    "        cross_attention_layers: int = 16,\n",
    "        attention_head_count: int = 8,\n",
    "        input_embedding_size: int = 256,\n",
    "        block_size: int = 60,\n",
    "        feature_length: int = 90,\n",
    "        timesteps: int = 300,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Transformer(encoder_layers, input_embedding_size, input_embedding_size * 2, input_embedding_size, attention_head_count, dropout)\n",
    "        self.decoder = Transformer(decoder_layers, input_embedding_size, input_embedding_size * 2, input_embedding_size, attention_head_count, dropout)\n",
    "        self.cross_attention = Transformer(cross_attention_layers, input_embedding_size, input_embedding_size * 2, input_embedding_size, attention_head_count, dropout)\n",
    "\n",
    "        self.positional_embedding = nn.Embedding(block_size, input_embedding_size)\n",
    "        self.time_embedding = nn.Embedding(timesteps, input_embedding_size)\n",
    "        self.feature_embedding = nn.Linear(feature_length, input_embedding_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, c: torch.Tensor, c_i: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        # x = (b, block_size, feature_length) c = (b, context_length, feature_length) c_i = (b, context_length,)   t = (b,)\n",
    "\n",
    "        time_embd = self.time_embedding(t).unsqueeze(1)\n",
    "\n",
    "        x_feature_embd = self.feature_embedding(x)\n",
    "        x_position_embd = self.positional_embedding(torch.arange(x.size(1)))\n",
    "\n",
    "        c_feature_embd = self.feature_embedding(c)\n",
    "        c_position_embd = self.positional_embedding(c_i)\n",
    "\n",
    "        x = x_feature_embd + x_position_embd + time_embd\n",
    "        c = c_feature_embd + c_position_embd + time_embd\n",
    "\n",
    "        c = self.encoder(c, mask=False)\n",
    "        x = self.decoder(x, mask=True)\n",
    "        x = self.cross_attention(x, c, mask=False)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def linear_beta_schedule(timesteps: int):\n",
    "    scale = 1000 / timesteps\n",
    "    start = scale * 0.0001\n",
    "    end = scale * 0.02\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b = t.shape[0]\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1, ) * (len(x_shape) - 1)))\n",
    "\n",
    "\n",
    "class Diffusion(nn.Module):\n",
    "    def __init__(self, timesteps: int):\n",
    "        super().__init__()\n",
    "        betas = linear_beta_schedule(timesteps)\n",
    "        alphas = 1. - betas\n",
    "\n",
    "        alphas_cumprod = torch.cumprod(alphas, 0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.)\n",
    "\n",
    "        self.register_buffer(\"betas\", betas)\n",
    "        self.register_buffer(\"alphas_cumprod\", alphas_cumprod)\n",
    "        self.register_buffer(\"alphas_cumprod_prev\", alphas_cumprod_prev)\n",
    "        self.register_buffer(\"sqrt_alphas_cumprod\", torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer(\"sqrt_one_minus_alphas_cumprod\",\n",
    "                             torch.sqrt(1. - alphas_cumprod))\n",
    "        \n",
    "        self.denoiser = Denoiser()\n",
    "\n",
    "    def forward_diffusion_sample(self, x_0, t):\n",
    "        noise = torch.randn_like(x_0)\n",
    "        return extract(self.sqrt_alphas_cumprod, t, x_0.shape) * x_0 + extract(self.sqrt_one_minus_alphas_cumprod, t, x_0.shape) * noise, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.97 Mb\n"
     ]
    }
   ],
   "source": [
    "diffusion = Diffusion(400)\n",
    "\n",
    "count = 0\n",
    "for p in diffusion.parameters():\n",
    "    count += p.numel()\n",
    "print(f\"{count * 4 / 1024 / 1024:.2f} Mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 60, 90])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.randn(64, 10, 90)\n",
    "c_i = torch.randint(0, 60, (64, 10,))   \n",
    "x = torch.randn(64, 60, 90)\n",
    "t = torch.randint(0, 300, (64,))\n",
    "\n",
    "\n",
    "# encoded_input = pose_encoder(input_poses, mask=False)\n",
    "# decoded_poses = pose_decoder(initial_noise)\n",
    "# output = cross_attention(decoded_poses, encoder_input=encoded_input, mask=False)\n",
    "\n",
    "\n",
    "# a = diffusion.denoiser(x, c, c_i, t)\n",
    "\n",
    "diffusion.forward_diffusion_sample(x, t)[0].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
